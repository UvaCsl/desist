{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload modules if source is modified \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import enum \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `easyvvuq` dependencies\n",
    "import easyvvuq as vvuq\n",
    "from easyvvuq.decoders import YAMLDecoder\n",
    "import chaospy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `isct` dependencies \n",
    "from workflow.uq import ISCTEncoder, ISCTDecoder\n",
    "from workflow.isct_trial import trial as trial_cmd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User directories: \n",
    "- `template_dir`: this directory points towards a patient directory that acts as template, i.e. this directory (and all its content) are duplicated for each new run generated by `EasyVVUQ`. Therefore, it is suggested to perform one analysis of that patient directory before generating the new directories with `EasyVVUQ`, as all initialisations and preprocessing is simply copied, rather then needed to be repeated. For example, create and run a single patient:\n",
    "```\n",
    "isct trial create one -n 1 \n",
    "isct trial run one -v \n",
    "```\n",
    "\n",
    "- `work_dir`: this is the directory in which `EasyVVUQ` will create the runs and corresponding databases. Everytime `EasyVVUQ` is started it generates a new database inside this folder (with a rather obscure random hash attached to the directory name). Be sure to clear out this `work_dir` somewhat regularly, as many directories can accumulate taking up some diskspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dir = \"/Users/max/trials/one/patient_000\"\n",
    "work_dir = \"/Users/max/trials/vvuq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure input is present\n",
    "template_dir = pathlib.Path(template_dir)\n",
    "assert os.path.isdir(template_dir)\n",
    "\n",
    "work_dir = pathlib.Path(work_dir)\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "assert os.path.isdir(work_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Campaing definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table lists a copy of the considered variables for UQ analysis of the one-dimensional bloodflow model. Each variable should be updated through `EasyVVUQ` and requires Encoders/Decoders to do so. The column `supported` indicates whether this is possible yes or no. The file names are all relative to the patient directory, i.e `trial/patient_i`.\n",
    "\n",
    "These are all _inputs_ towards the bloodflow model, where various outputs are possible, e.g. the flow or related properties for a variety of arteries. TODO: what are the output fields of interest, i.e. which artery and what physical property?\n",
    "\n",
    "| variable `name` | type | unit | location | supported | range | \n",
    "| ------ | ------ | ------ | ------ | ----- | ----- | \n",
    "| heart rate `HeartRate` | uncertain | bmp | `patient.yml`, `config.xml` | yes | N(68,20) | \n",
    "| stroke volume `StrokeVolume` | uncertain | ml | `bf_sim/Model_parameters.txt` | yes | N(104,21) | \n",
    "| blood density `Density` | uncertain | kg.m-3 | `bf_sim/Model_parameters.txt` | yes | U(1019,1061)|\n",
    "| blood viscosity `BLOOD_VISC` | uncertain | mPa.s | `bf_sim/Model_parameters.txt` | yes | N(62.9,18.1) |\n",
    "| wall thickness | uncertain | mm | per vessel: `1-D_Anatomy.txt` | no | N(0.44,0.04) |\n",
    "| wall elasticity | uncertain | mmHg | per vessel: `1-D_Anatomy.txt` | no | N(951,380) |\n",
    "| vertebral artery diameter | uncertain | mm | `unknown` | no | U(3.2,6.5) |\n",
    "| systolic pressure `SystolePressure` | certain | mmHg | `bf_sim/Model_parameters.txt` | yes | kept to value from WP2 (`rr_syst`) |\n",
    "| diastolic pressure `DiastolePressure` | certain | mmHg | `bf_sim/Model_parameters.txt` | yes | kept to hardcoded default value as in `workflow/patient.py` |\n",
    "| mean right atrial pressure | certain | mmHg | `unknown` | no | |  \n",
    "| clot location | certain | categorical | `Clots.txt` | no | kept to hardcoded value `R. MCA` as in `workflow/patient.py` until issue #48 is resolved. Should be considered as discrete distribution |\n",
    "| CoW vessel diameters | certain | mm | `unknown` | no | |\n",
    "| CoW vessel lengths | certain | mm | `unknown` | no | |\n",
    "| brain mesh | certain | mm | `unknown` | no | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Unclear if yet possible to switch brain meshes.\n",
    "- Many data on vessel diamaters: which to vary for UQ?\n",
    "- Elasticity appears in `1-D_Anatomy.txt` for each vessel: which to vary for UQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a `campaign`: effectively a directory containing a SQL database \n",
    "# and the directories for the various analysis. \n",
    "campaign = vvuq.Campaign('UQ_', work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a `encoder`: encode the parameters towards input files\n",
    "encoder = ISCTEncoder(template_fname=template_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file where `pressure drop` data is written to \n",
    "output = \"bf_sim/ResultsPerVessel.csv\"\n",
    "\n",
    "# output variables of interest in `ResultPerVessel.csv`\n",
    "cols = [\"VolumeFlowrate(mL/s)\", \"Pressure(Pa)\"]\n",
    "\n",
    "# create a `decoder`: decode the output parameters towards the database\n",
    "decoder = vvuq.decoders.SimpleCSV(target_filename=output, output_columns=cols, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a `collater` to aggregate the output data \n",
    "collater = vvuq.collate.AggregateSamples(average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of interest and their properties\n",
    "# this all just goes into a single dictionary, where now only \n",
    "# `BLOOD_VISC` is considered as parameter to be varied \n",
    "parameters = {\n",
    "    \"HeartRate\": { \n",
    "        \"type\": \"float\", \n",
    "        \"min\": 0,\n",
    "        \"max\": 200,\n",
    "        \"default\": 68,\n",
    "    },\n",
    "    \"StrokeVolume\": {\n",
    "        \"type\": \"float\",\n",
    "        \"min\": 0,\n",
    "        \"max\": 250,\n",
    "        \"default\": 104,\n",
    "    },\n",
    "    #\"Density\": {\n",
    "    #    \"type\": \"float\",\n",
    "    #    \"min\": 0.0,\n",
    "    #    \"max\": 3000,\n",
    "    #    \"default\": 1019,\n",
    "    #},\n",
    "    \"BLOOD_VISC\": {\n",
    "        \"type\": \"float\", \n",
    "       \"min\": 0.0, \n",
    "        \"max\": 1.0, \n",
    "    \"default\": 0.035,\n",
    "    },\n",
    "    #\"SystolePressure\": { # current left to its default value\n",
    "    #    \"type\": \"float\",\n",
    "    #    \"min\": 40,\n",
    "    #    \"max\": 220,\n",
    "    #    \"default\": 100,\n",
    "    #}\n",
    "    #\"DiastolePressure\": { \n",
    "    #    \"type\": \"float\",\n",
    "    #    \"min\": 40,\n",
    "    #    \"max\": 220,\n",
    "    #    \"default\": 100,\n",
    "    #}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an `app` for the campaign, by connecting all components\n",
    "campaign.add_app(\n",
    "    name=\"blood-visc\",\n",
    "    params=parameters,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    collater=collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters to vary are provided as dict with their \n",
    "# corresponding distributions \n",
    "vary = {\n",
    "    \"HeartRate\": cp.Normal(68, 20), \n",
    "    \"StrokeVolume\": cp.Normal(104, 21),\n",
    "    #\"Density\": cp.Uniform(1019,1061),\n",
    "    #\"BLOOD_VISC\": cp.Normal(0.0629, 0.0181),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available methods\n",
    "class Method(enum.Enum):\n",
    "    random = \"random\"\n",
    "    PCE = \"PCE\"\n",
    "    QMC = \"QMC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick any from the Enum \n",
    "method = Method.QMC\n",
    "\n",
    "# create a `sampler` matching the method \n",
    "if method == method.random: \n",
    "    sampler = vvuq.sampling.RandomSampler(vary=vary)\n",
    "    \n",
    "if method == method.PCE: \n",
    "    sampler = vvuq.sampling.PCESampler(vary=vary, polynomial_order=3)\n",
    "    \n",
    "if method == method.QMC:\n",
    "    sampler = vvuq.sampling.QMCSampler(vary=vary, n_mc_samples=10**2) # this is the default\n",
    "\n",
    "# assign the sampler\n",
    "campaign.set_sampler(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `num_samples` variable seems to act as either a limit or indication of the desired number of samples to be drawn. For the more advanced methods, PCE and QMC, it seems most logical to set the number of samples sufficiently high, such that PCE/QMK can dictate the required number of samples to draw. Note, if PCE/QMC are restricted to too little samples, the corresponding analysis might not be able to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the samples\n",
    "num_samples = 100000\n",
    "replicas = 1 # the number of times a single sample is replicated\n",
    "campaign.draw_samples(num_samples=num_samples, replicas=replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This logs all the runs in the current `campaign`. It is mostly for \n",
    "# simple inspection to see if the desired parameters are varied and to \n",
    "# list all runs. \n",
    "for run in campaign.list_runs():\n",
    "    print(f\"{run[1]['run_name']}: {run[1]['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all run directories; copies the template and updates the\n",
    "# parameters using the `ISCTEncoder`\n",
    "campaign.populate_runs_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `isct` for each proposed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By populating the campaign, all required subdirectories are created in the directory of the database. These directories represent patient directories for which various ways are available to evaluate their simulations. We can consider the base directory as a trial directory, and invoke `isct trial run` to evaluate the simulations of all subdirectories. Alternatively, and more involved, we could manually invoke the individual runs by `isct patient run`. Eitherway, the required steps are evaluated and the output is stored within the individual run directories. Afterwards, the collation step will aggregate these results back into the database. \n",
    "\n",
    "To run the jobs in parallel, we can do so locally by exploiting parallel with `n` procs (`-jn`)\n",
    "\n",
    "`isct trial run {run_dir} --gnu-parallel | parallel -jn`\n",
    "\n",
    "TODO:\n",
    "- support running notebook on external workstation: evacuate jobs locally on the remote machine\n",
    "- support remote execution on workstations: send the jobs towards the remote workstation for execution\n",
    "- support remote execution on HPC systems: send the jobs through a queing system to HPC systems \n",
    "- investigate efficient collation: archive data sets remotely, transport only essential information for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the path from the database location, this seems required\n",
    "# to obtain the hash that is attached after the original directory \n",
    "run_dir = campaign.db_location.split(\":\")[-1]\n",
    "run_dir = pathlib.Path(run_dir).parent\n",
    "\n",
    "# the runs are located in the /runs/ directory\n",
    "run_dir = run_dir.joinpath(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either run this command:\n",
    "#trial_cmd(f\"trial run {run_dir} -v\".split())\n",
    "\n",
    "# evaluate the following command in a subprocess\n",
    "#import subprocess\n",
    "#subprocess.run(f\"isct trial run {run_dir} -v\".split())\n",
    "\n",
    "# or run the following output in terminal \n",
    "print(f\"isct trial run {run_dir} -v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting output\n",
    "This steps collects the output parameters of interest from the output files and stores the output in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign.collate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing output\n",
    "- The analysis has to match the sampling method, these are directly related.\n",
    "- The analysis seems to fail for PCE/QMC when too little samples are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define analysis in line with sampling method\n",
    "if method == method.random:\n",
    "    analysis = vvuq.analysis.BasicStats(\n",
    "        qoi_cols=cols\n",
    "    )\n",
    "\n",
    "if method == method.PCE: \n",
    "    analysis = vvuq.analysis.PCEAnalysis(\n",
    "        sampler=sampler, \n",
    "        qoi_cols=cols\n",
    "    )\n",
    "    \n",
    "if method == method.QMC:\n",
    "    analysis = vvuq.analysis.QMCAnalysis(\n",
    "        sampler=sampler,\n",
    "        qoi_cols=cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply analysis to current database\n",
    "campaign.apply_analysis(analysis)\n",
    "results = campaign.get_last_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results are reported as a dictionary, where the contents are strongly dependent on the chosen sampling and analysis method. Both PCE and QMC report Sobol indices in addition to basic statistical information. From here, we can either perform the post-processing directly on the obtained dictionary, however, direct data analysis on the database is also a possibility. Note, it seems to make sense to exploit the already provided analysis methods provided in EasyVVUQ as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dictionary keys depend on the analysis\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information obtained from the collation can be accessed explicitly. \n",
    "# This returns a panda dataframe with all the data fields, this can be\n",
    "# used for any type of analysis as well.\n",
    "hr = campaign.get_collation_result()[4::225]['HeartRate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and saving results\n",
    "The state of the campaign can be stored explicitly and from there reloaded elsewhere. This should provided the necessary functionality to serialise the current state of the campaign, together with the database and all subdirectories, and move this archive between systems. Thus, we can move around the database and the runs of all samples between systems, e.g. between remote and local machines.\n",
    "\n",
    "The `state.json` simply contains the type of samplers, collation, and aggregation methods as well as the details of the database, i.e. its path, and the working directory that contains all subdirectories of the the individual samples. From there, we can initialise a new campaign and continue where previously left of. Thus, the data analysis could be decoupled completely from the scripts that perform the VVUQ analysis, which also saves recomputation compared to reevaluating the database over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = work_dir.joinpath(\"state.json\")\n",
    "campaign.save_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_campaign = vvuq.Campaign(state_file=state, work_dir=work_dir)\n",
    "reloaded_campaign.get_collation_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
