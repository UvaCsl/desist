{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload modules if source is modified \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chaospy as cp\n",
    "import easyvvuq as vvuq\n",
    "import enum \n",
    "import os\n",
    "import pathlib\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isct.trial import Trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path where Singularity containers are stored\n",
    "# provide `None` to use Docker\n",
    "container_path = None\n",
    "# container_path = pathlib.Path('/containers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for a reference trial and the VVUQ database(s)\n",
    "trial_dir = pathlib.Path('/Users/max/trials/singleton')\n",
    "datab_dir = pathlib.Path('/Users/max/trials/vvuq')\n",
    "\n",
    "# the template points to the first patient directory\n",
    "template_dir = pathlib.Path('/Users/max/trials/singleton/patient_00000')\n",
    "template_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub(path):\n",
    "    subdir = map(lambda p: os.path.join(path, p), os.listdir(path))\n",
    "    return list(filter(os.path.isdir, subdir))\n",
    "\n",
    "def dir_to_dict(dir):\n",
    "    if sub(dir) == []: \n",
    "        return None\n",
    "    return {os.path.basename(p): dir_to_dict(p) for p in sub(dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory encoder\n",
    "tree = dir_to_dict(template_dir)\n",
    "dir_enc = vvuq.encoders.DirectoryBuilder(tree=tree)\n",
    "dir_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the YAML encoder (still a custom class)\n",
    "# TODO: might want to submit a pull request for this encoder?\n",
    "from easyvvuq.encoders import BaseEncoder\n",
    "import yaml\n",
    "class YAMLEncoder(BaseEncoder, encoder_name='yaml_encoder'):\n",
    "    def __init__(self, template_filename, target_filename=None):\n",
    "        self.template_filename = str(template_filename)\n",
    "        if target_filename:\n",
    "            self.target_filename = target_filename\n",
    "        else:\n",
    "            self.target_filename = os.path.basename(self.template_filename)\n",
    "        \n",
    "    def encode(self, params={}, target_dir=''):\n",
    "        if not target_dir: \n",
    "            raise RuntimeError('No target directory specified to encode')\n",
    "            \n",
    "        try:\n",
    "            with open(self.template_filename, 'r') as template_file:\n",
    "                yaml_file = yaml.safe_load(template_file)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\n",
    "            f'the template file specified ({self.template_filename}) does not exist')\n",
    "        \n",
    "        # Extract parameters that are actually present in the\n",
    "        # configuration file already. These are the subset of\n",
    "        # parameters that we update during encoding\n",
    "        matched_params = {k:v for (k, v) in params.items() if k in yaml_file}\n",
    "            \n",
    "        yaml_file.update(matched_params)\n",
    "        target_file_path = os.path.join(target_dir, self.target_filename)\n",
    "        with open(target_file_path, 'w') as fp:\n",
    "            yaml.dump(yaml_file, fp)\n",
    "            \n",
    "    def get_restart_dict(self):\n",
    "        return {\"target_filename\": self.target_filename,\n",
    "                \"template_filename\": self.template_filename}\n",
    "    \n",
    "    def element_version(self):\n",
    "        return \"0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The blood flow is not a common file format, it is given \n",
    "# as a `key=value\\n` text file. We can cook up a specific\n",
    "# class to handle these cases, but we can also do a little \n",
    "# trick..., we can construct a template from the parameters\n",
    "# we want to modify\n",
    "\n",
    "# and then, we use that template file with the already\n",
    "# provided generic encoders\n",
    "\n",
    "def bf_processor(original_fn, template_fn, params):\n",
    "    # process line by line, replace `key=value` by `key=$key`\n",
    "    with open(template_fn, 'w') as template:\n",
    "        with open(original_fn, 'r') as original:\n",
    "            for line in original:\n",
    "                key, value = line.strip().split('=')\n",
    "                if key in params:\n",
    "                    # replace by template if present in parameters\n",
    "                    value = f'${key}'\n",
    "                template.write(f'{key}={value}\\n')\n",
    "\n",
    "def create_generic_encoder(processor, params, template_dir, filepath, delimiter='$'):\n",
    "    from easyvvuq.encoders import GenericEncoder\n",
    "    \n",
    "    # setup the input/output paths\n",
    "    original_fn = template_dir.joinpath(filepath)\n",
    "    base, _ = os.path.splitext(filepath)  # split original extension\n",
    "    template_fn = template_dir.joinpath(f'{base}.template')\n",
    "    \n",
    "    # evaluate the processor function\n",
    "    processor(original_fn, template_fn, params)\n",
    "    \n",
    "    # setup the encoder \n",
    "    encoder = GenericEncoder(str(template_fn), delimiter=delimiter, target_filename=str(filepath))\n",
    "    return encoder\n",
    "    \n",
    "bf_enc = create_generic_encoder(bf_processor, {'BLOOD_VISC', 'Density'}, template_dir, 'baseline/bf_sim/Model_parameters.txt')     \n",
    "bf_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the patient configuration encoder\n",
    "patient_enc = YAMLEncoder(template_dir.joinpath('patient.yml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table lists a copy of the considered variables for UQ analysis of the one-dimensional bloodflow model. Each variable should be updated through `EasyVVUQ` and requires Encoders/Decoders to do so. The column `supported` indicates whether this is possible yes or no. The file names are all relative to the patient directory, i.e `trial/patient_i`.\n",
    "\n",
    "These are all _inputs_ towards the bloodflow model, where various outputs are possible, e.g. the flow or related properties for a variety of arteries. TODO: what are the output fields of interest, i.e. which artery and what physical property?\n",
    "\n",
    "| variable `name` | type | unit | location | supported | range | \n",
    "| ------ | ------ | ------ | ------ | ----- | ----- | \n",
    "| heart rate `HeartRate` | uncertain | bmp | `patient.yml`, `config.xml` | yes | N(68,20) | \n",
    "| stroke volume `StrokeVolume` | uncertain | ml | `bf_sim/Model_parameters.txt` | yes | N(104,21) | \n",
    "| blood density `Density` | uncertain | kg.m-3 | `bf_sim/Model_parameters.txt` | yes | U(1019,1061)|\n",
    "| blood viscosity `BLOOD_VISC` | uncertain | mPa.s | `bf_sim/Model_parameters.txt` | yes | N(62.9,18.1) |\n",
    "| wall thickness | uncertain | mm | per vessel: `1-D_Anatomy.txt` | no | N(0.44,0.04) |\n",
    "| wall elasticity | uncertain | mmHg | per vessel: `1-D_Anatomy.txt` | no | N(951,380) |\n",
    "| vertebral artery diameter | uncertain | mm | `unknown` | no | U(3.2,6.5) |\n",
    "| systolic pressure `SystolePressure` | certain | mmHg | `bf_sim/Model_parameters.txt` | yes | kept to value from WP2 (`rr_syst`) |\n",
    "| diastolic pressure `DiastolePressure` | certain | mmHg | `bf_sim/Model_parameters.txt` | yes | kept to hardcoded default value as in `workflow/patient.py` |\n",
    "| mean right atrial pressure | certain | mmHg | `unknown` | no | |  \n",
    "| clot location | certain | categorical | `Clots.txt` | no | kept to hardcoded value `R. MCA` as in `workflow/patient.py` until issue #48 is resolved. Should be considered as discrete distribution |\n",
    "| CoW vessel diameters | certain | mm | `unknown` | no | |\n",
    "| CoW vessel lengths | certain | mm | `unknown` | no | |\n",
    "| brain mesh | certain | mm | `unknown` | no | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Unclear if yet possible to switch brain meshes.\n",
    "- Many data on vessel diamaters: which to vary for UQ?\n",
    "- Elasticity appears in `1-D_Anatomy.txt` for each vessel: which to vary for UQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a `campaign`: effectively a directory containing a SQL database\n",
    "# each campain is formed by the prefix with a unique identifier as postfix\n",
    "prefix = 'UQ_'\n",
    "campaign = vvuq.Campaign(prefix, work_dir=datab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_copy = [\n",
    "    template_dir.joinpath('Clots.txt'),\n",
    "    template_dir.joinpath('baseline/1-D_Anatomy.txt'),\n",
    "    template_dir.joinpath('baseline/CoW_Complete.txt'),\n",
    "    template_dir.joinpath('baseline/1-D_Anatomy_Patient.txt'),\n",
    "]\n",
    "\n",
    "for path in os.listdir(template_dir.joinpath('baseline')):\n",
    "    path = template_dir.joinpath('baseline').joinpath(path)\n",
    "    if os.path.isfile(path):\n",
    "        files_to_copy.append(path)      \n",
    "\n",
    "targets = [p.relative_to(template_dir) for p in files_to_copy]\n",
    "copy_enc = [vvuq.encoders.CopyEncoder(str(src), str(dst)) for src, dst in zip(files_to_copy, targets)]\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vvuq.encoders.MultiEncoder(*[dir_enc, *copy_enc, patient_enc, bf_enc])\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file where `pressure drop` data is written to \n",
    "# TODO: this should probably be provided somewhere as constants\n",
    "output = \"baseline/bf_sim/ResultsPerVessel.csv\"\n",
    "\n",
    "# output variables of interest in `ResultPerVessel.csv`\n",
    "cols = [\"VolumeFlowrate(mL/s)\", \"Pressure(Pa)\"]\n",
    "\n",
    "# create a `decoder`: decode the output parameters towards the database\n",
    "decoder = vvuq.decoders.SimpleCSV(target_filename=output, output_columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of interest and their properties\n",
    "# this all just goes into a single dictionary, where now only \n",
    "# `BLOOD_VISC` is considered as parameter to be varied \n",
    "parameters = {\n",
    "    #\"HeartRate\": { \n",
    "    #    \"type\": \"float\", \n",
    "    #    \"min\": 0,\n",
    "    #    \"max\": 200,\n",
    "    #    \"default\": 73,\n",
    "    #},\n",
    "    \"StrokeVolume\": {\n",
    "        \"type\": \"float\",\n",
    "        \"min\": 0,\n",
    "        \"max\": 250,\n",
    "        \"default\": 95,\n",
    "    },\n",
    "    \"Density\": {\n",
    "        \"type\": \"float\",\n",
    "        \"min\": 0.0,\n",
    "        \"max\": 3000,\n",
    "        \"default\": 1050,\n",
    "    },\n",
    "    \"BLOOD_VISC\": {\n",
    "        \"type\": \"float\", \n",
    "       \"min\": 0.0, \n",
    "        \"max\": 10.0e-3, \n",
    "    \"default\": 4.2e-3,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an `app` for the campaign, by connecting all components\n",
    "campaign.add_app(\n",
    "    name=\"blood-visc\",\n",
    "    params=parameters,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters to vary are provided as dict with their \n",
    "# corresponding distributions \n",
    "vary = {\n",
    "    #\"HeartRate\": cp.Normal(73, 12.2), \n",
    "    \"StrokeVolume\": cp.Normal(95, 14),\n",
    "    \"Density\": cp.Uniform(1040,1055),\n",
    "    \"BLOOD_VISC\": cp.Normal(4.2e-3, 0.9e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available methods\n",
    "class Method(enum.Enum):\n",
    "    random = \"random\"\n",
    "    PCE = \"PCE\"\n",
    "    QMC = \"QMC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick any from the Enum \n",
    "method = Method.QMC\n",
    "\n",
    "# create a `sampler` matching the method \n",
    "if method == method.random: \n",
    "    sampler = vvuq.sampling.RandomSampler(vary=vary)\n",
    "    \n",
    "if method == method.PCE: \n",
    "    sampler = vvuq.sampling.PCESampler(vary=vary, polynomial_order=3)\n",
    "    \n",
    "if method == method.QMC:\n",
    "    sampler = vvuq.sampling.QMCSampler(vary=vary, n_mc_samples=10**4) # this is the default\n",
    "\n",
    "# assign the sampler\n",
    "campaign.set_sampler(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `num_samples` variable seems to act as either a limit or indication of the desired number of samples to be drawn. For the more advanced methods, PCE and QMC, it seems most logical to set the number of samples sufficiently high, such that PCE/QMK can dictate the required number of samples to draw. Note, if PCE/QMC are restricted to too little samples, the corresponding analysis might not be able to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the samples\n",
    "num_samples = 500\n",
    "replicas = 1 # the number of times a single sample is replicated\n",
    "campaign.draw_samples(num_samples=num_samples, replicas=replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This logs all the runs in the current `campaign`. It is mostly for \n",
    "# simple inspection to see if the desired parameters are varied and to \n",
    "# list all runs. \n",
    "for run in campaign.list_runs():\n",
    "    print(f\"{run[1]['run_name']}: {run[1]['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all run directories; copies the template and updates the\n",
    "# parameters using the `ISCTEncoder`\n",
    "campaign.populate_runs_dir()\n",
    "state = datab_dir.joinpath(\"state.json\")\n",
    "campaign.save_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = campaign.db_location.split(\":\")[-1]\n",
    "run_dir = pathlib.Path(run_dir).parent\n",
    "run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_dir.joinpath('runs/trial.yml'), 'w') as config_file:\n",
    "    trial_config = {\n",
    "        'container-path': '/scratch/containers'\n",
    "    }\n",
    "    yaml.dump(trial_config, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (run, _) in campaign.list_runs():\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(campaign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `isct` for each proposed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By populating the campaign, all required subdirectories are created in the directory of the database. These directories represent patient directories for which various ways are available to evaluate their simulations. We can consider the base directory as a trial directory, and invoke `isct trial run` to evaluate the simulations of all subdirectories. Alternatively, and more involved, we could manually invoke the individual runs by `isct patient run`. Eitherway, the required steps are evaluated and the output is stored within the individual run directories. Afterwards, the collation step will aggregate these results back into the database. \n",
    "\n",
    "To run the jobs in parallel, we can do so locally by exploiting parallel with `n` procs (`-jn`)\n",
    "\n",
    "`isct trial run {run_dir} --gnu-parallel | parallel -jn`\n",
    "\n",
    "TODO:\n",
    "- support running notebook on external workstation: evacuate jobs locally on the remote machine\n",
    "- support remote execution on workstations: send the jobs towards the remote workstation for execution\n",
    "- support remote execution on HPC systems: send the jobs through a queing system to HPC systems \n",
    "- investigate efficient collation: archive data sets remotely, transport only essential information for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the path from the database location, this seems required\n",
    "# to obtain the hash that is attached after the original directory \n",
    "run_dir = campaign.db_location.split(\":\")[-1]\n",
    "run_dir = pathlib.Path(run_dir).parent\n",
    "\n",
    "# the runs are located in the /runs/ directory\n",
    "run_dir = run_dir.joinpath(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two approaches to running the trial\n",
    "\n",
    "# 1. from the cmd-line manually \n",
    "# evaluate with `-x` to log the commands, without `-x` to\n",
    "# actually run the commands\n",
    "print(f\"isct -v trial run {run_dir} -x --clean-files\")\n",
    "\n",
    "# 2. from the notebook directly\n",
    "# select the `Logger()` to log the commands to output\n",
    "# select the `LocalRunner()` to run the commands locally\n",
    "# this evaluates the run commands per patient in a subshell\n",
    "#from isct.runner import Logger, LocalRunner\n",
    "#runner = Logger()\n",
    "# runner = LocalRunner()\n",
    "#keep_files = False  # remove large output files\n",
    "\n",
    "#for patient in Trial(path=run_dir, runner=runner, keep_files=keep_files):\n",
    "#    patient.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting output\n",
    "This steps collects the output parameters of interest from the output files and stores the output in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign.collate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing output\n",
    "- The analysis has to match the sampling method, these are directly related.\n",
    "- The analysis seems to fail for PCE/QMC when too little samples are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define analysis in line with sampling method\n",
    "if method == method.random:\n",
    "    analysis = vvuq.analysis.BasicStats(\n",
    "        qoi_cols=cols\n",
    "    )\n",
    "\n",
    "if method == method.PCE: \n",
    "    analysis = vvuq.analysis.PCEAnalysis(\n",
    "        sampler=sampler, \n",
    "        qoi_cols=cols\n",
    "    )\n",
    "    \n",
    "if method == method.QMC:\n",
    "    analysis = vvuq.analysis.QMCAnalysis(\n",
    "        sampler=sampler,\n",
    "        qoi_cols=cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply analysis to current database\n",
    "campaign.apply_analysis(analysis)\n",
    "results = campaign.get_last_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results are reported as a dictionary, where the contents are strongly dependent on the chosen sampling and analysis method. Both PCE and QMC report Sobol indices in addition to basic statistical information. From here, we can either perform the post-processing directly on the obtained dictionary, however, direct data analysis on the database is also a possibility. Note, it seems to make sense to exploit the already provided analysis methods provided in EasyVVUQ as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dictionary keys depend on the analysis\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information obtained from the collation can be accessed explicitly. \n",
    "# This returns a panda dataframe with all the data fields, this can be\n",
    "# used for any type of analysis as well.\n",
    "hr = campaign.get_collation_result()[4::225]['Pressure(Pa)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and saving results\n",
    "The state of the campaign can be stored explicitly and from there reloaded elsewhere. This should provided the necessary functionality to serialise the current state of the campaign, together with the database and all subdirectories, and move this archive between systems. Thus, we can move around the database and the runs of all samples between systems, e.g. between remote and local machines.\n",
    "\n",
    "The `state.json` simply contains the type of samplers, collation, and aggregation methods as well as the details of the database, i.e. its path, and the working directory that contains all subdirectories of the the individual samples. From there, we can initialise a new campaign and continue where previously left of. Thus, the data analysis could be decoupled completely from the scripts that perform the VVUQ analysis, which also saves recomputation compared to reevaluating the database over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyvvuq.decoders import YAMLDecoder\n",
    "\n",
    "class ISCTDecoder(YAMLDecoder, decoder_name=\"ISCTDecoder\"):\n",
    "    \"\"\"ISCTDecoder decodes the patient outcome from isct trials.\n",
    "\n",
    "    Currently the implementation wraps a `YAMLDecoder` to extract the patient\n",
    "    outcome from the `patient_outcome.yml` generated by the `patient-outcome`\n",
    "    module.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 target_filename='patient_outcome.yml',\n",
    "                 output_columns=None):\n",
    "        super().__init__(target_filename, output_columns)\n",
    "\n",
    "    def sim_complete(self, run_info=None):\n",
    "        \"\"\"Returns True if the target file is present.\n",
    "\n",
    "        When considering `patient_outcome.yml` as target file, the result of\n",
    "        sim_complete coincides with an indication if the simulation was\n",
    "        complete, as the file is only present afterwards.\n",
    "        \"\"\"\n",
    "        assert run_info is not None, \"No run information provided.\"\n",
    "        return os.patoh.isfile(\n",
    "            pathlib.Path(run_info['run_dir']).joinpath(self.target_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = datab_dir.joinpath(\"state.json\")\n",
    "campaign.save_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = datab_dir.joinpath(\"state.json\")\n",
    "reloaded_campaign = vvuq.Campaign(state_file=state, work_dir=datab_dir)\n",
    "reloaded_campaign.get_collation_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decoder that retrieves the result after reloading the database\n",
    "# this specifically decodes the output of the blood-flow model \n",
    "decoder = vvuq.decoders.SimpleCSV(target_filename=output, output_columns=cols)\n",
    "run = run_dir.joinpath('Run_1')\n",
    "decoder.parse_sim_output({'run_dir':run})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly we can define a decoder for the patient outcome, this can be done \n",
    "# using the ISCTDecoder, a wrapper around a YAMLDecoder, that extracts the \n",
    "# data from all `patient_outcome.yml` files for the different runs in the \n",
    "# database results\n",
    "output = \"patient_outcome.yml\"\n",
    "\n",
    "# accessing output columns of interest from the YAML format can be achieved by \n",
    "# providing the root of the variable, while for nested dictionaries, we can \n",
    "# provide the comlete tree [root, node, ..., node, leaf] that is traversed,\n",
    "# where the value of the resuling dictionary [root][node][...][leaf] is \n",
    "# extracted and returned in the output dataframe\n",
    "cols = [\"sex\", \"age\", [\"thrombectomy\", \"result\"], ['infarct-volume', 'stroke', '11', 'volume']]\n",
    "decoder = ISCTDecoder(target_filename=output, output_columns=cols)\n",
    "decoder.parse_sim_output({'run_dir':run})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
